TITLE: Make Executable and Run Python Example
DESCRIPTION: These commands first make a Python example script executable and then run it directly against the configured API, demonstrating how to test custom examples within the project's environment.
SOURCE: https://github.com/openai/openai-python/blob/main/CONTRIBUTING.md#_snippet_5

LANGUAGE: sh
CODE:
```
chmod +x examples/<your-example>.py
```

LANGUAGE: sh
CODE:
```
./examples/<your-example>.py
```

----------------------------------------

TITLE: Connect to Realtime API and Send Text Messages
DESCRIPTION: Provides an example of connecting to the OpenAI Realtime API using the `beta.realtime.connect` method. It demonstrates how to update session modalities and send user text input, then process streaming text responses and handle completion events.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_10

LANGUAGE: Python
CODE:
```
import asyncio
from openai import AsyncOpenAI

async def main():
    client = AsyncOpenAI()

    async with client.beta.realtime.connect(model="gpt-4o-realtime-preview") as connection:
        await connection.session.update(session={'modalities': ['text']})

        await connection.conversation.item.create(
            item={
                "type": "message",
                "role": "user",
                "content": [{"type": "input_text", "text": "Say hello!"}],
            }
        )
        await connection.response.create()

        async for event in connection:
            if event.type == 'response.text.delta':
                print(event.delta, flush=True, end="")

            elif event.type == 'response.text.done':
                print()

            elif event.type == "response.done":
                break

asyncio.run(main())
```

----------------------------------------

TITLE: OpenAI Completions API: Create Completion
DESCRIPTION: This API documentation describes the method for creating text completions using the OpenAI API. It details the endpoint, the client method signature, and the expected return type, allowing developers to generate text based on provided parameters.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_2

LANGUAGE: APIDOC
CODE:
```
client.completions.create(**params) -> Completion
  - Endpoint: POST /completions
  - Description: Creates a new text completion.
  - Parameters:
    - **params: Dictionary of parameters for completion creation (e.g., model, prompt, max_tokens).
  - Returns: A Completion object containing the generated text and metadata.
```

----------------------------------------

TITLE: Customize OpenAI Client HTTP Options Per-Request
DESCRIPTION: This example illustrates how to apply specific `httpx` client configurations on a per-request basis using the `client.with_options()` method. This is useful for dynamic adjustments to HTTP behavior without re-initializing the main client.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_33

LANGUAGE: python
CODE:
```
client.with_options(http_client=DefaultHttpxClient(...))
```

----------------------------------------

TITLE: OpenAI Client Core Methods and Realtime API Reference
DESCRIPTION: Comprehensive documentation for key methods available on the OpenAI Python client, including chat completions, streaming responses, and detailed specifications for the Realtime API connection, session management, message sending, and event types with their attributes.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_12

LANGUAGE: APIDOC
CODE:
```
OpenAI.chat.completions.create(messages: list, model: str, ...)
  - Creates a chat completion.
  - Parameters:
    - messages: A list of message objects, e.g., [{'role': 'user', 'content': '...'}]
    - model: The model to use (e.g., "gpt-4o").
  - Returns: A ChatCompletion object.

OpenAI.responses.create(model: str, input: str, stream: bool, ...)
  - Creates a response, optionally streaming.
  - Parameters:
    - model: The model to use.
    - input: The input text for the response generation.
    - stream: If true, an iterable of events is returned for streaming.
  - Returns: A Response object or an asynchronous iterable of events if streaming.

AsyncOpenAI.beta.realtime.connect(model: str, ...)
  - Establishes a WebSocket connection to the Realtime API.
  - Parameters:
    - model: The Realtime API model to use (e.g., "gpt-4o-realtime-preview").
  - Returns: An asynchronous context manager for the connection object.

RealtimeConnection.session.update(session: dict)
  - Updates the session configuration for the Realtime API connection.
  - Parameters:
    - session: A dictionary containing session configuration, e.g., {'modalities': ['text']}.

RealtimeConnection.conversation.item.create(item: dict)
  - Sends an item (e.g., a message) to the conversation within the Realtime API.
  - Parameters:
    - item: A dictionary representing the item, e.g., {"type": "message", "role": "user", "content": [...]}.

RealtimeConnection.response.create()
  - Initiates a response from the model within the Realtime API session.

RealtimeEvent Types:
  - 'response.text.delta': Partial text response received.
    - Attributes: delta (str) - The incremental text received.
  - 'response.text.done': Full text response completed.
  - 'response.done': Overall response completed (e.g., all text and audio done).
  - 'error': An error occurred during the Realtime API interaction.
    - Attributes:
      - error.type (str): The type of error.
      - error.code (str): The error code.
      - error.event_id (str): A unique ID for the error event.
      - error.message (str): A descriptive error message.
```

----------------------------------------

TITLE: Generate Text with OpenAI Responses API in Python
DESCRIPTION: Demonstrates how to use the OpenAI Python client to generate text using the new Responses API. It initializes the client with an API key from environment variables and sends a prompt to the 'gpt-4o' model, then prints the generated output.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_1

LANGUAGE: python
CODE:
```
import os
from openai import OpenAI

client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)

response = client.responses.create(
    model="gpt-4o",
    instructions="You are a coding assistant that talks like a pirate.",
    input="How do I check if a Python object is an instance of a class?",
)

print(response.output_text)
```

----------------------------------------

TITLE: Handle OpenAI API Errors in Python
DESCRIPTION: This example illustrates how to implement robust error handling for OpenAI API calls using `try-except` blocks. It demonstrates catching specific error types such as `APIConnectionError` for network issues, `RateLimitError` for 429 responses, and `APIStatusError` for other non-success HTTP status codes, allowing access to `status_code` and `response` properties for detailed debugging.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_21

LANGUAGE: python
CODE:
```
import openai
from openai import OpenAI

client = OpenAI()

try:
    client.fine_tuning.jobs.create(
        model="gpt-4o",
        training_file="file-abc123",
    )
except openai.APIConnectionError as e:
    print("The server could not be reached")
    print(e.__cause__)  # an underlying Exception, likely raised within httpx.
except openai.RateLimitError as e:
    print("A 429 status code was received; we should back off a bit.")
except openai.APIStatusError as e:
    print("Another non-200-range status code was received")
    print(e.status_code)
    print(e.response)
```

----------------------------------------

TITLE: Perform Asynchronous API Calls with AsyncOpenAI in Python
DESCRIPTION: Illustrates how to use the `AsyncOpenAI` client for asynchronous interactions with the OpenAI API. It initializes the client, defines an async function to make a `responses.create` call, and runs it using `asyncio.run`.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_5

LANGUAGE: python
CODE:
```
import os
import asyncio
from openai import AsyncOpenAI

client = AsyncOpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get("OPENAI_API_KEY"),
)


async def main() -> None:
    response = await client.responses.create(
        model="gpt-4o", input="Explain disestablishmentarianism to a smart five year old."
    )
    print(response.output_text)


asyncio.run(main())
```

----------------------------------------

TITLE: OpenAI Python Client Batches API Methods
DESCRIPTION: Details the client methods for managing batch operations with the OpenAI API, covering creation, retrieval, listing, and cancellation of batches.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_58

LANGUAGE: APIDOC
CODE:
```
client.batches.create(**params)
  - Description: Creates a new batch job.
  - HTTP Endpoint: POST /batches
  - Returns: Batch type object

client.batches.retrieve(batch_id)
  - Description: Retrieves a specific batch job by its ID.
  - HTTP Endpoint: GET /batches/{batch_id}
  - Returns: Batch type object

client.batches.list(**params)
  - Description: Lists all batch jobs.
  - HTTP Endpoint: GET /batches
  - Returns: SyncCursorPage[Batch] type object

client.batches.cancel(batch_id)
  - Description: Cancels a specific batch job.
  - HTTP Endpoint: POST /batches/{batch_id}/cancel
  - Returns: Batch type object
```

----------------------------------------

TITLE: Add New Python Example File
DESCRIPTION: This snippet shows the typical shebang for a new Python example file within the `examples/` directory, indicating it should be run via Rye. The `examples/` directory is not modified by the generator, allowing free edits.
SOURCE: https://github.com/openai/openai-python/blob/main/CONTRIBUTING.md#_snippet_4

LANGUAGE: py
CODE:
```
#!/usr/bin/env -S rye run python
â€¦
```

----------------------------------------

TITLE: Set Up Mock Server for API Testing
DESCRIPTION: This command uses `npx prism` to set up a mock server based on an OpenAPI specification. This mock server is required for running most tests against the API, simulating its behavior.
SOURCE: https://github.com/openai/openai-python/blob/main/CONTRIBUTING.md#_snippet_9

LANGUAGE: sh
CODE:
```
npx prism mock path/to/your/openapi.yml
```

----------------------------------------

TITLE: Add API Endpoint: Get Response Input Items
DESCRIPTION: Introduces a new API endpoint (`GET /responses/{response_id}/input_items`) to retrieve input items associated with a specific response ID. This enhances the ability to inspect the original data used for a given API response.
SOURCE: https://github.com/openai/openai-python/blob/main/CHANGELOG.md#_snippet_16

LANGUAGE: APIDOC
CODE:
```
GET /responses/{response_id}/input_items
```

----------------------------------------

TITLE: OpenAI Python Client Uploads API Methods
DESCRIPTION: Outlines the client methods for managing file uploads to the OpenAI API, including operations for initiating, cancelling, and completing uploads.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_60

LANGUAGE: APIDOC
CODE:
```
client.uploads.create(**params)
  - Description: Initiates a new file upload.
  - HTTP Endpoint: POST /uploads
  - Returns: Upload type object

client.uploads.cancel(upload_id)
  - Description: Cancels an ongoing file upload.
  - HTTP Endpoint: POST /uploads/{upload_id}/cancel
  - Returns: Upload type object

client.uploads.complete(upload_id, **params)
  - Description: Completes a multi-part file upload.
  - HTTP Endpoint: POST /uploads/{upload_id}/complete
  - Returns: Upload type object
```

----------------------------------------

TITLE: OpenAI Thread Run Steps API Methods
DESCRIPTION: API documentation for interacting with individual steps within an OpenAI thread run, including methods for retrieving a specific step by its ID and listing all steps for a given run.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_54

LANGUAGE: APIDOC
CODE:
```
GET /threads/{thread_id}/runs/{run_id}/steps/{step_id}
  client.beta.threads.runs.steps.retrieve(step_id, *, thread_id, run_id, **params) -> RunStep

GET /threads/{thread_id}/runs/{run_id}/steps
  client.beta.threads.runs.steps.list(run_id, *, thread_id, **params) -> SyncCursorPage[RunStep]
```

----------------------------------------

TITLE: OpenAI Beta Assistants API Methods
DESCRIPTION: Documents the core API methods for managing OpenAI Assistants via the Python client. This includes operations for creating, retrieving, updating, listing, and deleting assistants, along with their respective HTTP endpoints and return types.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_48

LANGUAGE: APIDOC
CODE:
```
POST /assistants
client.beta.assistants.create(**params) -> Assistant

GET /assistants/{assistant_id}
client.beta.assistants.retrieve(assistant_id) -> Assistant

POST /assistants/{assistant_id}
client.beta.assistants.update(assistant_id, **params) -> Assistant

GET /assistants
client.beta.assistants.list(**params) -> SyncCursorPage[Assistant]

DELETE /assistants/{assistant_id}
client.beta.assistants.delete(assistant_id) -> AssistantDeleted
```

----------------------------------------

TITLE: Add API Endpoint: Get Chat Completions
DESCRIPTION: Adds a new API endpoint (`GET /chat/completions`) for retrieving chat completions. This provides a direct method to access chat completion results via the API.
SOURCE: https://github.com/openai/openai-python/blob/main/CHANGELOG.md#_snippet_17

LANGUAGE: APIDOC
CODE:
```
GET /chat/completions
```

----------------------------------------

TITLE: OpenAI Python Client Upload Parts API Methods
DESCRIPTION: Describes the client method for creating individual parts of a multi-part file upload to the OpenAI API.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_62

LANGUAGE: APIDOC
CODE:
```
client.uploads.parts.create(upload_id, **params)
  - Description: Creates a new part for a multi-part file upload.
  - HTTP Endpoint: POST /uploads/{upload_id}/parts
  - Returns: UploadPart type object
```

----------------------------------------

TITLE: OpenAI Fine-tuning Alpha Graders API Methods
DESCRIPTION: Documents the API methods available for interacting with the fine-tuning alpha graders, including running and validating grader operations. Each method details its HTTP endpoint, client call signature, and expected return type.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_32

LANGUAGE: APIDOC
CODE:
```
POST /fine_tuning/alpha/graders/run
  client.fine_tuning.alpha.graders.run(**params) -> GraderRunResponse

POST /fine_tuning/alpha/graders/validate
  client.fine_tuning.alpha.graders.validate(**params) -> GraderValidateResponse
```

----------------------------------------

TITLE: OpenAI Python Client Responses API Methods
DESCRIPTION: Provides an overview of the core API methods for interacting with response objects in the OpenAI Python client. This includes operations for creating, retrieving, deleting, and canceling responses.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_64

LANGUAGE: APIDOC
CODE:
```
POST /responses
client.responses.create(**params) -> Response
  - Creates a new response object.
  - Parameters:
    - params: Additional parameters for response creation (e.g., input, format).
  - Returns: The created Response object.

GET /responses/{response_id}
client.responses.retrieve(response_id, **params) -> Response
  - Retrieves a specific response object by its ID.
  - Parameters:
    - response_id: The unique identifier of the response.
    - params: Additional parameters for retrieval.
  - Returns: The retrieved Response object.

DELETE /responses/{response_id}
client.responses.delete(response_id) -> None
  - Deletes a specific response object by its ID.
  - Parameters:
    - response_id: The unique identifier of the response to delete.
  - Returns: None upon successful deletion.

POST /responses/{response_id}/cancel
client.responses.cancel(response_id) -> Response
  - Cancels an in-progress response.
  - Parameters:
    - response_id: The unique identifier of the response to cancel.
  - Returns: The canceled Response object.
```

----------------------------------------

TITLE: Upload files to OpenAI API using Python client
DESCRIPTION: Explains how to upload files to the OpenAI API using the Python client. The `file` parameter accepts `bytes`, `PathLike` instances, or a tuple of `(filename, contents, media type)` for flexible file handling.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_18

LANGUAGE: python
CODE:
```
from pathlib import Path
from openai import OpenAI

client = OpenAI()

client.files.create(
    file=Path("input.jsonl"),
    purpose="fine-tune",
)
```

----------------------------------------

TITLE: Handle Assistant API Streaming Events with Custom Class
DESCRIPTION: Provides a comprehensive example of creating a custom `AssistantEventHandler` class to subscribe to and process various events (text, tool calls) from the OpenAI Assistant API stream, demonstrating method overrides for specific event types.
SOURCE: https://github.com/openai/openai-python/blob/main/helpers.md#_snippet_6

LANGUAGE: python
CODE:
```
from typing_extensions import override
from openai import AssistantEventHandler, OpenAI
from openai.types.beta.threads import Text, TextDelta
from openai.types.beta.threads.runs import ToolCall, ToolCallDelta

client = openai.OpenAI()

# First, we create a EventHandler class to define
# how we want to handle the events in the response stream.

class EventHandler(AssistantEventHandler):
  @override
  def on_text_created(self, text: Text) -> None:
    print(f"\nassistant > ", end="", flush=True)

  @override
  def on_text_delta(self, delta: TextDelta, snapshot: Text):
    print(delta.value, end="", flush=True)

  @override
  def on_tool_call_created(self, tool_call: ToolCall):
    print(f"\nassistant > {tool_call.type}\n", flush=True)

  @override
  def on_tool_call_delta(self, delta: ToolCallDelta, snapshot: ToolCall):
    if delta.type == "code_interpreter" and delta.code_interpreter:
      if delta.code_interpreter.input:
        print(delta.code_interpreter.input, end="", flush=True)
      if delta.code_interpreter.outputs:
        print(f"\n\noutput >", flush=True)
        for output in delta.code_interpreter.outputs:
          if output.type == "logs":
            print(f"\n{output.logs}", flush=True)

# Then, we use the `stream` SDK helper
# with the `EventHandler` class to create the Run
# and stream the response.

with client.beta.threads.runs.stream(
  thread_id="thread_id",
  assistant_id="assistant_id",
  event_handler=EventHandler(),
) as stream:
  stream.until_done()
```

----------------------------------------

TITLE: OpenAI Files API: Client Methods
DESCRIPTION: Provides a comprehensive overview of the client methods available for managing files within the OpenAI platform, including operations for creation, retrieval, listing, deletion, and content access.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_10

LANGUAGE: APIDOC
CODE:
```
client.files.create(**params) -> FileObject
  HTTP Method: POST
  Path: /files
  Description: Uploads a file to OpenAI.
  Parameters:
    params: Dictionary of parameters for file creation, including file content and purpose.

client.files.retrieve(file_id) -> FileObject
  HTTP Method: GET
  Path: /files/{file_id}
  Description: Retrieves information about a specific file by its ID.
  Parameters:
    file_id: The ID of the file to retrieve.

client.files.list(**params) -> SyncCursorPage[FileObject]
  HTTP Method: GET
  Path: /files
  Description: Lists all files uploaded by the organization.
  Parameters:
    params: Optional dictionary of parameters for filtering the list of files.

client.files.delete(file_id) -> FileDeleted
  HTTP Method: DELETE
  Path: /files/{file_id}
  Description: Deletes a specific file by its ID.
  Parameters:
    file_id: The ID of the file to delete.

client.files.content(file_id) -> HttpxBinaryResponseContent
  HTTP Method: GET
  Path: /files/{file_id}/content
  Description: Retrieves the raw content of a specific file.
  Parameters:
    file_id: The ID of the file whose content is to be retrieved.

client.files.retrieve_content(file_id) -> str
  HTTP Method: GET
  Path: /files/{file_id}/content
  Description: Retrieves the content of a specific file as a string.
  Parameters:
    file_id: The ID of the file whose content is to be retrieved.

client.files.wait_for_processing(*args) -> FileObject
  Description: Waits for a file to finish processing on the OpenAI platform.
  Parameters:
    args: Arguments passed to the waiting mechanism.
```

----------------------------------------

TITLE: Creating and Streaming a Thread Run (OpenAI Python)
DESCRIPTION: This method creates a thread, starts a run, and streams real-time updates as the run progresses. It returns an `AssistantStreamManager` which allows for event-driven processing of the run's lifecycle.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_97

LANGUAGE: Python
CODE:
```
client.beta.threads.create_and_run_stream(*args)
```

----------------------------------------

TITLE: OpenAI Python Client Containers API Reference
DESCRIPTION: Detailed API reference for managing containers using the OpenAI Python client. This section outlines methods for creating new containers, retrieving existing ones by ID, listing all available containers, and deleting containers.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_75

LANGUAGE: APIDOC
CODE:
```
POST /containers
client.containers.create(**params) -> ContainerCreateResponse
  - Creates a new container.
  - Parameters:
    - **params: Dictionary of parameters for container creation (e.g., name, configuration).
  - Returns: ContainerCreateResponse object upon successful creation.

GET /containers/{container_id}
client.containers.retrieve(container_id) -> ContainerRetrieveResponse
  - Retrieves a specific container by its unique identifier.
  - Parameters:
    - container_id: The ID of the container to retrieve.
  - Returns: ContainerRetrieveResponse object containing the container's details.

GET /containers
client.containers.list(**params) -> SyncCursorPage[ContainerListResponse]
  - Lists all containers, with support for pagination and filtering.
  - Parameters:
    - **params: Optional dictionary of parameters for listing containers (e.g., limit, after, before).
  - Returns: A paginated list (SyncCursorPage) of ContainerListResponse objects.

DELETE /containers/{container_id}
client.containers.delete(container_id) -> None
  - Deletes a specific container by its unique identifier.
  - Parameters:
    - container_id: The ID of the container to delete.
  - Returns: None upon successful deletion.
```

----------------------------------------

TITLE: Creating a Run for a Thread (OpenAI Python)
DESCRIPTION: This method initiates a new execution run for a specified `thread_id`. It accepts `params` to configure the run, such as the Assistant to use, and returns a `Run` object representing the newly started execution.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_99

LANGUAGE: Python
CODE:
```
client.beta.threads.runs.create(thread_id, **params)
```

----------------------------------------

TITLE: OpenAI Python Client Input Items API Methods
DESCRIPTION: Provides the API method for listing input items associated with a specific response in the OpenAI Python client. This allows for the retrieval of paginated collections of response input details.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_66

LANGUAGE: APIDOC
CODE:
```
GET /responses/{response_id}/input_items
client.responses.input_items.list(response_id, **params) -> SyncCursorPage[ResponseItem]
  - Lists input items associated with a specific response.
  - Parameters:
    - response_id: The unique identifier of the response.
    - params: Additional parameters for listing input items (e.g., pagination, filters).
  - Returns: A paginated collection (SyncCursorPage) of ResponseItem objects.
```

----------------------------------------

TITLE: Manage OpenAI Fine-tuning Jobs
DESCRIPTION: API methods for creating, retrieving, listing, canceling, pausing, and resuming fine-tuning jobs within the OpenAI platform.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_26

LANGUAGE: APIDOC
CODE:
```
POST /fine_tuning/jobs
client.fine_tuning.jobs.create(**params) -> FineTuningJob

GET /fine_tuning/jobs/{fine_tuning_job_id}
client.fine_tuning.jobs.retrieve(fine_tuning_job_id) -> FineTuningJob

GET /fine_tuning/jobs
client.fine_tuning.jobs.list(**params) -> SyncCursorPage[FineTuningJob]

POST /fine_tuning/jobs/{fine_tuning_job_id}/cancel
client.fine_tuning.jobs.cancel(fine_tuning_job_id) -> FineTuningJob

GET /fine_tuning/jobs/{fine_tuning_job_id}/events
client.fine_tuning.jobs.list_events(fine_tuning_job_id, **params) -> SyncCursorPage[FineTuningJobEvent]

POST /fine_tuning/jobs/{fine_tuning_job_id}/pause
client.fine_tuning.jobs.pause(fine_tuning_job_id) -> FineTuningJob

POST /fine_tuning/jobs/{fine_tuning_job_id}/resume
client.fine_tuning.jobs.resume(fine_tuning_job_id) -> FineTuningJob
```

----------------------------------------

TITLE: OpenAI Models API: Client Methods
DESCRIPTION: Provides client methods for interacting with OpenAI models, including operations for retrieving model details, listing available models, and deleting fine-tuned models.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_23

LANGUAGE: APIDOC
CODE:
```
client.models.retrieve(model) -> Model
  HTTP Method: GET
  Path: /models/{model}
  Description: Retrieves information about a specific model.
  Parameters:
    model: The ID of the model to retrieve.

client.models.list() -> SyncPage[Model]
  HTTP Method: GET
  Path: /models
  Description: Lists all available models.
  Parameters: None.

client.models.delete(model
```

----------------------------------------

TITLE: Verify and parse OpenAI webhooks with Python Flask
DESCRIPTION: Provides a Flask application example demonstrating how to use `client.webhooks.unwrap()` to verify the signature and parse the payload of an incoming OpenAI webhook. It handles different event types and raises an error for invalid signatures.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_19

LANGUAGE: python
CODE:
```
from openai import OpenAI
from flask import Flask, request

app = Flask(__name__)
client = OpenAI()  # OPENAI_WEBHOOK_SECRET environment variable is used by default


@app.route("/webhook", methods=["POST"])
def webhook():
    request_body = request.get_data(as_text=True)

    try:
        event = client.webhooks.unwrap(request_body, request.headers)

        if event.type == "response.completed":
            print("Response completed:", event.data)
        elif event.type == "response.failed":
            print("Response failed:", event.data)
        else:
            print("Unhandled event type:", event.type)

        return "ok"
    except Exception as e:
        print("Invalid signature:", e)
        return "Invalid signature", 400


if __name__ == "__main__":
    app.run(port=8000)
```

----------------------------------------

TITLE: Configure Automatic Retries for OpenAI API Requests in Python
DESCRIPTION: This section explains the automatic retry mechanism in the OpenAI Python SDK for transient errors like connection issues, timeouts, conflicts, rate limits, and server errors. It provides examples of how to configure the `max_retries` option, either globally for all client requests or on a per-request basis using `with_options()`, allowing fine-grained control over retry behavior.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_24

LANGUAGE: python
CODE:
```
from openai import OpenAI

# Configure the default for all requests:
client = OpenAI(
    # default is 2
    max_retries=0,
)

# Or, configure per-request:
client.with_options(max_retries=5).chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How can I get the name of the current day in JavaScript?",
        }
    ],
    model="gpt-4o",
)
```

----------------------------------------

TITLE: OpenAI Thread Runs API Methods
DESCRIPTION: Comprehensive API documentation for managing OpenAI thread runs, including methods for creation, retrieval, updating, listing, cancellation, and submitting tool outputs. It also covers polling and streaming functionalities for asynchronous operations.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_52

LANGUAGE: APIDOC
CODE:
```
POST /threads/{thread_id}/runs
  client.beta.threads.runs.create(thread_id, **params) -> Run

GET /threads/{thread_id}/runs/{run_id}
  client.beta.threads.runs.retrieve(run_id, *, thread_id) -> Run

POST /threads/{thread_id}/runs/{run_id}
  client.beta.threads.runs.update(run_id, *, thread_id, **params) -> Run

GET /threads/{thread_id}/runs
  client.beta.threads.runs.list(thread_id, **params) -> SyncCursorPage[Run]

POST /threads/{thread_id}/runs/{run_id}/cancel
  client.beta.threads.runs.cancel(run_id, *, thread_id) -> Run

POST /threads/{thread_id}/runs/{run_id}/submit_tool_outputs
  client.beta.threads.runs.submit_tool_outputs(run_id, *, thread_id, **params) -> Run

client.beta.threads.runs.create_and_poll(*args) -> Run

client.beta.threads.runs.create_and_stream(*args) -> AssistantStreamManager[AssistantEventHandler] | AssistantStreamManager[AssistantEventHandlerT]

client.beta.threads.runs.poll(*args) -> Run

client.beta.threads.runs.stream(*args) -> AssistantStreamManager[AssistantEventHandler] | AssistantStreamManager[AssistantEventHandlerT]

client.beta.threads.runs.submit_tool_outputs_and_poll(*args) -> Run

client.beta.threads.runs.submit_tool_outputs_stream(*args) -> AssistantStreamManager[AssistantEventHandler] | AssistantStreamManager[AssistantEventHandlerT]
```

----------------------------------------

TITLE: OpenAI Embeddings API: Create Embedding
DESCRIPTION: This API documentation describes the method for generating vector embeddings from input text using the OpenAI API. It outlines the endpoint, the client method signature, and the expected return type, enabling the conversion of text into numerical representations for various AI tasks.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_8

LANGUAGE: APIDOC
CODE:
```
client.embeddings.create(**params) -> CreateEmbeddingResponse
  - Endpoint: POST /embeddings
  - Description: Creates an embedding vector representing the input text.
  - Parameters:
    - **params: Dictionary of parameters for embedding creation (e.g., input text, model).
  - Returns: A CreateEmbeddingResponse object containing the embedding.
```

----------------------------------------

TITLE: Analyze Image from URL with OpenAI Vision API in Python
DESCRIPTION: Illustrates how to use the OpenAI Vision API to analyze an image provided via a URL. It constructs a user message with both text and an image URL, then sends it to the 'gpt-4o-mini' model for analysis.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_3

LANGUAGE: python
CODE:
```
prompt = "What is in this image?"
img_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/2023_06_08_Raccoon1.jpg/1599px-2023_06_08_Raccoon1.jpg"

response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {
            "role": "user",
            "content": [
                {"type": "input_text", "text": prompt},
                {"type": "input_image", "image_url": f"{img_url}"},
            ],
        }
    ],
)
```

----------------------------------------

TITLE: OpenAI Beta Realtime Sessions API Methods
DESCRIPTION: This section describes the API methods available for managing sessions within the OpenAI beta realtime feature. It currently includes the `create` method for initiating new real-time sessions, allowing applications to establish and control interactive audio or text streams.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_44

LANGUAGE: APIDOC
CODE:
```
client.beta.realtime.sessions.create(**params) -> SessionCreateResponse
  - Creates a new realtime session.
  - Parameters:
    - **params: Arbitrary keyword arguments for session creation parameters.
  - Returns: A SessionCreateResponse object.
```

----------------------------------------

TITLE: Integrate Responses with Evals API in OpenAI
DESCRIPTION: Integrates responses with the Evals API, suggesting improvements in how evaluation results are handled or generated in conjunction with API responses.
SOURCE: https://github.com/openai/openai-python/blob/main/CHANGELOG.md#_snippet_13

LANGUAGE: APIDOC
CODE:
```
API Update:
  - Responses integrated with Evals API.
```

----------------------------------------

TITLE: Pass nested parameters in OpenAI Python API calls
DESCRIPTION: Illustrates how to provide nested parameters, such as the `input` array for chat completions, as Python dictionaries when making API requests. This is common for complex request bodies.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_17

LANGUAGE: python
CODE:
```
from openai import OpenAI

client = OpenAI()

response = client.chat.responses.create(
    input=[
        {
            "role": "user",
            "content": "How much ?",
        }
    ],
    model="gpt-4o",
    response_format={"type": "json_object"},
)
```

----------------------------------------

TITLE: OpenAI Python Client Container Files API Reference
DESCRIPTION: Detailed API reference for managing files within specific containers using the OpenAI Python client. This section covers methods for creating, retrieving, listing, and deleting files associated with a given container.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_76

LANGUAGE: APIDOC
CODE:
```
POST /containers/{container_id}/files
client.containers.files.create(container_id, **params) -> FileCreateResponse
  - Creates a new file within a specified container.
  - Parameters:
    - container_id: The ID of the container where the file will be created.
    - **params: Dictionary of parameters for file creation (e.g., file content, name).
  - Returns: FileCreateResponse object upon successful creation.

GET /containers/{container_id}/files/{file_id}
client.containers.files.retrieve(file_id, *, container_id) -> FileRetrieveResponse
  - Retrieves a specific file by its ID from a specified container.
  - Parameters:
    - file_id: The ID of the file to retrieve.
    - container_id: The ID of the container the file belongs to.
  - Returns: FileRetrieveResponse object containing the file's details.

GET /containers/{container_id}/files
client.containers.files.list(container_id, **params) -> SyncCursorPage[FileListResponse]
  - Lists all files within a specified container, with support for pagination and filtering.
  - Parameters:
    - container_id: The ID of the container whose files are to be listed.
    - **params: Optional dictionary of parameters for listing files.
  - Returns: A paginated list (SyncCursorPage) of FileListResponse objects.

DELETE /containers/{container_id}/files/{file_id}
client.containers.files.delete(file_id, *, container_id) -> None
  - Deletes a specific file by its ID from a specified container.
  - Parameters:
    - file_id: The ID of the file to delete.
    - container_id: The ID of the container the file belongs to.
  - Returns: None upon successful deletion.
```

----------------------------------------

TITLE: Stream OpenAI API Responses
DESCRIPTION: Demonstrates how to stream API responses using `.with_streaming_response` within a context manager. This approach allows for iterative processing of the response body (e.g., line by line) and ensures the response is reliably closed.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_30

LANGUAGE: python
CODE:
```
with client.chat.completions.with_streaming_response.create(
    messages=[
        {
            "role": "user",
            "content": "Say this is a test",
        }
    ],
    model="gpt-4o",
) as response:
    print(response.headers.get("X-My-Header"))

    for line in response.iter_lines():
        print(line)
```

----------------------------------------

TITLE: OpenAI Beta Realtime Transcription Sessions API Methods
DESCRIPTION: This section outlines the API methods for managing transcription sessions within the OpenAI beta realtime feature. It includes the `create` method, which allows applications to initiate new real-time transcription sessions for processing audio input.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_46

LANGUAGE: APIDOC
CODE:
```
client.beta.realtime.transcription_sessions.create(**params) -> TranscriptionSession
  - Creates a new realtime transcription session.
  - Parameters:
    - **params: Arbitrary keyword arguments for transcription session creation parameters.
  - Returns: A TranscriptionSession object.
```

----------------------------------------

TITLE: Creating and Polling a Thread Run (OpenAI Python)
DESCRIPTION: This method creates a thread, starts a run, and then synchronously polls for the run's completion. It's a convenience for blocking operations, returning the final `Run` object once execution finishes.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_96

LANGUAGE: Python
CODE:
```
client.beta.threads.create_and_run_poll(*args)
```

----------------------------------------

TITLE: Generate Text with OpenAI Chat Completions API in Python
DESCRIPTION: Shows how to use the OpenAI Python client to generate text using the older, but still supported, Chat Completions API. It sends a list of messages (roles and content) to the 'gpt-4o' model and prints the first choice's content.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_2

LANGUAGE: python
CODE:
```
from openai import OpenAI

client = OpenAI()

completion = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "developer", "content": "Talk like a pirate."},
        {
            "role": "user",
            "content": "How do I check if a Python object is an instance of a class?",
        },
    ],
)

print(completion.choices[0].message.content)
```

----------------------------------------

TITLE: OpenAI API Error Types and Status Codes
DESCRIPTION: This documentation outlines the various error types raised by the OpenAI Python SDK and their corresponding HTTP status codes. It provides a quick reference for common API errors such as bad requests, authentication failures, permission issues, not found errors, unprocessable entities, rate limits, and internal server errors.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_22

LANGUAGE: APIDOC
CODE:
```
Error codes are as follows:

| Status Code | Error Type                 |
| ----------- | -------------------------- |
| 400         | `BadRequestError`          |
| 401         | `AuthenticationError`      |
| 403         | `PermissionDeniedError`    |
| 404         | `NotFoundError`            |
| 422         | `UnprocessableEntityError` |
| 429         | `RateLimitError`           |
| >=500       | `InternalServerError`      |
| N/A         | `APIConnectionError`       |
```

----------------------------------------

TITLE: OpenAI Evals API Endpoints
DESCRIPTION: Defines the core API methods for interacting with OpenAI Evals, including creation, retrieval, updating, listing, and deletion of evaluation resources. These methods allow programmatic management of evaluation configurations.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_68

LANGUAGE: APIDOC
CODE:
```
POST /evals
  Method: client.evals.create(**params)
  Returns: EvalCreateResponse
  Description: Creates a new evaluation.
  Parameters:
    - params: Dictionary of parameters for evaluation creation (e.g., data source, model, instructions).

GET /evals/{eval_id}
  Method: client.evals.retrieve(eval_id)
  Returns: EvalRetrieveResponse
  Description: Retrieves a specific evaluation by its ID.
  Parameters:
    - eval_id: The unique identifier of the evaluation to retrieve.

POST /evals/{eval_id}
  Method: client.evals.update(eval_id, **params)
  Returns: EvalUpdateResponse
  Description: Updates an existing evaluation.
  Parameters:
    - eval_id: The unique identifier of the evaluation to update.
    - params: Dictionary of parameters for evaluation update.

GET /evals
  Method: client.evals.list(**params)
  Returns: SyncCursorPage[EvalListResponse]
  Description: Lists all evaluations, with optional filtering and pagination.
  Parameters:
    - params: Optional parameters for filtering and pagination (e.g., limit, after, order).

DELETE /evals/{eval_id}
  Method: client.evals.delete(eval_id)
  Returns: EvalDeleteResponse
  Description: Deletes a specific evaluation by its ID.
  Parameters:
    - eval_id: The unique identifier of the evaluation to delete.
```

----------------------------------------

TITLE: OpenAI Containers API Methods
DESCRIPTION: Defines the API endpoints and corresponding client methods for managing top-level containers. This includes operations to create new containers, retrieve specific containers by ID, list all available containers, and delete containers.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_82

LANGUAGE: APIDOC
CODE:
```
client.containers.create(**params) -> ContainerCreateResponse (POST /containers)
client.containers.retrieve(container_id) -> ContainerRetrieveResponse (GET /containers/{container_id})
client.containers.list(**params) -> SyncCursorPage[ContainerListResponse] (GET /containers)
client.containers.delete(container_id) -> None (DELETE /containers/{container_id})
```

----------------------------------------

TITLE: Install aiohttp for Async OpenAI Client
DESCRIPTION: Installs the `aiohttp` library, which can be used as an alternative HTTP backend for the `AsyncOpenAI` client to potentially improve concurrency performance.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_6

LANGUAGE: sh
CODE:
```
pip install aiohttp
```

----------------------------------------

TITLE: Update Assistants and Evals API Schemas
DESCRIPTION: Updates the underlying schemas for the Assistants and Evals APIs. This ensures consistency and compatibility with the latest API definitions.
SOURCE: https://github.com/openai/openai-python/blob/main/CHANGELOG.md#_snippet_14

LANGUAGE: APIDOC
CODE:
```
API Schema Update:
  - Assistants API schema updated
  - Evals API schema updated
```

----------------------------------------

TITLE: Handle Errors in Realtime API Connection
DESCRIPTION: Demonstrates how to implement error handling for the OpenAI Realtime API. It shows how to check for `error` events within the connection's event stream and access detailed error attributes like type, code, event ID, and message.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_11

LANGUAGE: Python
CODE:
```
client = AsyncOpenAI()

async with client.beta.realtime.connect(model="gpt-4o-realtime-preview") as connection:
    ...
    async for event in connection:
        if event.type == 'error':
            print(event.error.type)
            print(event.error.code)
            print(event.error.event_id)
            print(event.error.message)
```

----------------------------------------

TITLE: OpenAI Beta Threads API Methods
DESCRIPTION: Documents the core API methods for managing OpenAI Threads via the Python client. This includes operations for creating, retrieving, updating, and deleting threads, as well as methods for creating and running threads, and handling streaming responses.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_50

LANGUAGE: APIDOC
CODE:
```
POST /threads
client.beta.threads.create(**params) -> Thread

GET /threads/{thread_id}
client.beta.threads.retrieve(thread_id) -> Thread

POST /threads/{thread_id}
client.beta.threads.update(thread_id, **params) -> Thread

DELETE /threads/{thread_id}
client.beta.threads.delete(thread_id) -> ThreadDeleted

POST /threads/runs
client.beta.threads.create_and_run(**params) -> Run

client.beta.threads.create_and_run_poll(*args) -> Run

client.beta.threads.create_and_run_stream(*args) -> AssistantStreamManager[AssistantEventHandler] | AssistantStreamManager[AssistantEventHandlerT]
```

----------------------------------------

TITLE: OpenAI Webhooks API Methods
DESCRIPTION: This section details the available methods for interacting with OpenAI webhooks. It includes functions for unwrapping webhook payloads and verifying their signatures, crucial for securely processing incoming webhook events from the OpenAI API.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_41

LANGUAGE: APIDOC
CODE:
```
client.webhooks.unwrap(payload, headers, *, secret) -> UnwrapWebhookEvent
  - Unwraps a webhook payload.
  - Parameters:
    - payload: The raw webhook payload.
    - headers: The request headers.
    - secret: The webhook secret for verification.
  - Returns: An UnwrapWebhookEvent object.

client.webhooks.verify_signature(payload, headers, *, secret, tolerance) -> None
  - Verifies the signature of a webhook payload.
  - Parameters:
    - payload: The raw webhook payload.
    - headers: The request headers.
    - secret: The webhook secret for verification.
    - tolerance: The time tolerance for signature verification.
  - Returns: None (raises an error if verification fails).
```

----------------------------------------

TITLE: OpenAI Vector Stores API Methods
DESCRIPTION: Documents the API methods for managing OpenAI Vector Stores, covering operations like creation, retrieval, update, listing, deletion, and searching. Each method includes its HTTP endpoint, client call signature, and expected return type.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_35

LANGUAGE: APIDOC
CODE:
```
POST /vector_stores
  client.vector_stores.create(**params) -> VectorStore

GET /vector_stores/{vector_store_id}
  client.vector_stores.retrieve(vector_store_id) -> VectorStore

POST /vector_stores/{vector_store_id}
  client.vector_stores.update(vector_store_id, **params) -> VectorStore

GET /vector_stores
  client.vector_stores.list(**params) -> SyncCursorPage[VectorStore]

DELETE /vector_stores/{vector_store_id}
  client.vector_stores.delete(vector_store_id) -> VectorStoreDeleted

POST /vector_stores/{vector_store_id}/search
  client.vector_stores.search(vector_store_id, **params) -> SyncPage[VectorStoreSearchResponse]
```

----------------------------------------

TITLE: Analyze Base64 Encoded Image with OpenAI Vision API in Python
DESCRIPTION: Demonstrates how to use the OpenAI Vision API to analyze an image provided as a base64 encoded string. It reads an image file, encodes it to base64, and then sends it along with a text prompt to the 'gpt-4o-mini' model.
SOURCE: https://github.com/openai/openai-python/blob/main/README.md#_snippet_4

LANGUAGE: python
CODE:
```
import base64
from openai import OpenAI

client = OpenAI()

prompt = "What is in this image?"
with open("path/to/image.png", "rb") as image_file:
    b64_image = base64.b64encode(image_file.read()).decode("utf-8")

response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {
            "role": "user",
            "content": [
                {"type": "input_text", "text": prompt},
                {"type": "input_image", "image_url": f"data:image/png;base64,{b64_image}"},
            ],
        }
    ],
)
```

----------------------------------------

TITLE: OpenAI Chat Completions API Methods
DESCRIPTION: This section details the primary methods for interacting with the OpenAI Chat Completions API. It covers operations such as creating new chat completions, retrieving existing ones, updating their state, listing multiple completions, and deleting them, providing a full lifecycle management for chat interactions.
SOURCE: https://github.com/openai/openai-python/blob/main/api.md#_snippet_5

LANGUAGE: APIDOC
CODE:
```
client.chat.completions.create(**params) -> ChatCompletion
  - Endpoint: POST /chat/completions
  - Description: Creates a new chat completion.
  - Parameters:
    - **params: Dictionary of parameters for chat completion creation (e.g., model, messages, temperature).
  - Returns: A ChatCompletion object.

client.chat.completions.retrieve(completion_id) -> ChatCompletion
  - Endpoint: GET /chat/completions/{completion_id}
  - Description: Retrieves a specific chat completion by its unique identifier.
  - Parameters:
    - completion_id: The unique ID of the chat completion to retrieve.
  - Returns: A ChatCompletion object.

client.chat.completions.update(completion_id, **params) -> ChatCompletion
  - Endpoint: POST /chat/completions/{completion_id}
  - Description: Updates an existing chat completion.
  - Parameters:
    - completion_id: The unique ID of the chat completion to update.
    - **params: Dictionary of parameters for updating the chat completion.
  - Returns: The updated ChatCompletion object.

client.chat.completions.list(**params) -> SyncCursorPage[ChatCompletion]
  - Endpoint: GET /chat/completions
  - Description: Lists chat completions, optionally filtered by parameters.
  - Parameters:
    - **params: Dictionary of parameters for filtering the list (e.g., limit, after).
  - Returns: A paginated list (SyncCursorPage) of ChatCompletion objects.

client.chat.completions.delete(completion_id) -> ChatCompletionDeleted
  - Endpoint: DELETE /chat/completions/{completion_id}
  - Description: Deletes a specific chat completion by its unique identifier.
  - Parameters:
    - completion_id: The unique ID of the chat completion to delete.
  - Returns: A ChatCompletionDeleted object indicating the deletion status.
```